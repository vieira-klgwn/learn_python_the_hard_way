import numpy as np


import pandas as pd


s = [1, 3, 5, np.nan, 6, 8]


s


dates = pd.date_range("20130101", periods=6)


dates


df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list("ABCD"))


df


df2 = pd.DataFrame({
    "A":1.0,
    "B":pd.Timestamp("20130102"),
    "C":pd.Series(1, index= list(range(4)), dtype="float32"),
    "D":np.array([3] * 4, dtype="int32"),
    "E":pd.Categorical(["test", "train", "test", "train"]),
    "F":"foo"
})


df2


df2.dtypes


df.tail()


df.head()


df.index


df.columns


df.to_numpy()


df2.dtypes


df2.to_numpy()


df.describe()


df.T


df.sort_index(axis = 1, ascending=False)


df


df.sort_index(axis=0, ascending=False)


df.sort_values(by="B")


df["A"]


df[0:0]


df[0:1]


df[0:2]





df[0:3]


df['20130102':'20130104']


df.loc[dates[0]]


df.loc[:, ["A", "B"]] #Selecting all rows (:) with a select column labels:


df.loc["20130102":"20130104", ["A", "B"]] # For label slicing, both endpoints are included:


df.loc[dates[0], "A"] #Selecting a single row and column label returns a scalar:


df.at[dates[0], "A"] #For getting fast access to a scalar (equivalent to the prior method):


df.iloc[3] #Select via the position of the passed integers:


df.iloc[3:5, 0:2] #Integer slices acts similar to NumPy/Python:


df.iloc[[1,2,4], [0, 2]] #Lists of integer position locations:


df.iloc[1:3, :] #For slicing rows explicitly


df.iloc[:, 1:3] #For slicing columns explicitly:


df.iloc[1, 1] #For getting a value explicitly


df.iat[1 ,1] # For getting fast access to a scalar # --equivalent to the prior method


# Boolean indexing


df[df["A"] > 0] #Select rows where df.A is greater than 0.


df[df > 0] #Selecting values from a DataFrame where a boolean condition is met:


# Using isin() method for filtering:


df2 = df.copy()


df2["E"] = ["one", "one", "two","three", "four", "three"]


df2


df2[df2["E"].isin(["two", "four"])]


# Settings


# Setting a new column automatically aligns the data by the indexes:


s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range("20130102", periods=6))


s1


df["F" ]= s1


df


df.at[dates[0], "A"] = 0 # Setting values by label:


df.iat[0, 1] = 0 # Setting values by position:


df


df.loc[:, "D"] = np.array([5] * len(df)) # Setting by assigning with a NumPy array:


len(df)


df


# A where operation with setting


df2 = df.copy()


df2[df2 > 0] = -df2


df2


# Miising data


df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ["E"])


df1.loc[dates[0] : dates[1] , "E"] = 1


df1


df # they jumped column called E in other dfs so we can add it in the newly df called df1


df1


df1.dropna(how="any") # drops any rows that have any


df1


df1.fillna(value=5) # fills missing data:


pd.isna(df1) # gets the boolean mask where values are nan:


df1.isna()


# Operations


df.mean()


df.mean(axis=1)


s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)


s


df.sub(s, axis="index") # I didn't understand what this is actually doing 


df.sub(s, axis="index")


# user defined functions


df.agg(lambda x: np.mean(x) * 5.6) 


df.transform(lambda x : x * 101.2)


# Value counts


s = pd.Series(np.random.randint(0, 7, size=10))


s.value_counts()


s


s = pd.Series(["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"])


s.str.lower()


# Merge


#concat


# concatinating pandas objects together row-wise with concat()


df = pd.DataFrame(np.random.randn(10, 4))


df


# break it into pieces


pieces = [df[:3], df[3:7], df[7:]]


pieces


pd.concat(pieces)


# join


left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})


right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})


left


right


pd.merge(left, right, on = "key")


# merge on unique keys


left = pd.DataFrame({"key": ["foo", "bar"], "lvar": [1,2]})


right = pd.DataFrame({"key": ["foo", "bar"], "rval":[4, 5]})


left


right


pd.merge(left, right, on="key")


# Grouping


df = pd.DataFrame({
    "A":["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"], 
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],

    "C": np.random.randn(8),

    "D": np.random.randn(8),
})


df


# Grouping by a column label, selecting column labels, and then applying the DataFrameGroupBy.sum() function to the resulting groups:


df.groupby("A")[["C", "D"]].sum()


# Grouping by multiple columns label forms MultiIndex.


df.groupby(["A", "B"]).sum()


# Reshaping


arrays = [

   ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],

   ["one", "two", "one", "two", "one", "two", "one", "two"],

]


index = pd.MultiIndex.from_arrays(arrays, names=["first", "second"])


df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=["A", "B"])


index


df


df2 = df[:4]


df2


# The stack() method “compresses” a level in the DataFrame’s columns:


stacked = df2.stack(future_stack=True)


stacked


# With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:


stacked.unstack()


stacked.unstack(1)


stacked.unstack(0)


# Pivot tables


df = pd.DataFrame(
    {
        
        "A": ["one", "one", "two", "three"] * 3,

        "B": ["A", "B", "C"] * 4,

        "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 2,

        "D": np.random.randn(12),

        "E": np.random.randn(12),
    }
)


df


# pivot_table() pivots a DataFrame specifying the values, index and columns


pd.pivot_table(df, values="D", index=["A", "B"], columns=["C"])



# Time series



